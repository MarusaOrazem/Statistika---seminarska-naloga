\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{eurosym}



\begin{document}
\begin{titlepage}
\begin{center}
       \vspace*{1cm}

       \textbf{POROČILO SEMINARSKE NALOGE}

       \vspace{0.5cm}

            
       \vspace{1.5cm}

       \text{MARUŠA ORAŽEM}

       \vfill
            
     
            
       Matematika\\
       Fakulteta za matematiko in fiziko\\
       \today
            
   \end{center}
\end{titlepage}

\section*{1. NALOGA}
Koda se nahaja v datoteki PrvaNaloga.ipynb, kjer so poračuunane vrednosti za spodnje primere. Pomagala sem si z knjižnico scipy.
\subsection*{a)}
%\subsubsection*{Povprečni dohodek}
%
%Populacija je velikosti $43886$. Vzamemo enostavni slučajni vzorec $400$ enot.\\
%Sledi: $N = 43886, n = 400$.\\
%Naj bo $X_k$ skupni dohodek v k-ti družini.\\
%Torej je povprečni dohodek v Kindergradu: 
%\begin{equation*}
%\overline{X} = \frac{X_1 + ... + X_n}{n}.
%\end{equation*}
%
%\subsubsection*{Standardna napaka}
%Vemo: $se( \overline{X} ) = \sqrt{var( \overline{X} )}$. Ker imamo enostavni slučajni vzorec, vemo tudi, da je
%\begin{equation*}
%var( \overline{X} ) = \frac{1}{n} \frac{N-n}{N-1} \sigma^2
%\end{equation*}
%kjer je $\sigma^2$ populacijska varjanca. Nepristranska cenilka za $\sigma^2$ je
%\begin{equation*}
%\hat{\sigma}^2 = \frac{N-1}{N(n-1)} \sum_{k=1}^{n}( X_k - \overline{X} )^2.
%\end{equation*}
%Sledi:
%\begin{equation*}
%\widehat{se(\overline{X})} = \sqrt{ \frac{1}{n} \frac{N-n}{N(n-1)} \sum_{k=1}^n (X_k - \overline{X} )^2.
%}
%\end{equation*}
%
%\subsubsection*{Interval zaupanja}
%Iz navodil sledi, da je interval zaupanja enak $\overline{X} \pm 1,96 \cdot se( \overline{X} )$.
%
%\subsubsection*{Končne vrednosti}
Iz datoteke PrvaNaloga.ipynb preberemo:\\
Ocena povprečnega dohodka v Kibergradu, glede na enostavno slučajno vzorčenje je 42942.15.\\
Standardna napaka: 1593.0009644380311.\\
Interval zaupanja: [39819.86810970146,46064.43189029854].

\subsection*{b)}

Če stratificiramo, mora veljati 
\begin{equation*}
\frac{n_k}{n} = \frac{N_k}{N},   \sum_{k=1}^k n_k = n .
\end{equation*}
V našem primeru startificiramo po četrtih, torej $k=4$.
Vemo:\\
$N_1 = 10 149$ (\textit{severna četrt}),\\
$N_2 = 10 390$ (\textit{vzgodna četrt}),\\
$N_3 = 13 457$ (\textit{južna četrt}),\\
$N_4 = 9 890$ (\textit{zahodna četrt}).
Če malo obrnemo zgornjo enakost, dobimo
\begin{equation*}
n_k = \frac{N_k}{N} n.
\end{equation*}
Izračunamo za $k = 1,2,3,4$ in upoštevamo vrednosti $N_1,N_2,N_3,N_4$ ter $N = 43886$.\\
Dobimo:
\begin{equation*}
n_1 = \frac{10149}{43886} \cdot 400 = 92,5033 \rightarrow n_1 = 92,
\end{equation*}

\begin{equation*}
n_2 = \frac{10390}{43886} \cdot 400 = 94, 699 \rightarrow n_2 = 95,
\end{equation*}

\begin{equation*}
n_3 = \frac{13457}{43886} \cdot 400 = 122,654 \rightarrow n_3 = 123,
\end{equation*}

\begin{equation*}
n_4 = \frac{9890}{43886} \cdot 400 = 90,142 \rightarrow n_4 = 90.
\end{equation*}

Preverimo:
\begin{equation*}
\sum_{k=1}^{4}n_k = 92+96+123+90 = 400.
\end{equation*}
%Naj bo sedaj $X_{kj}$ povprečni dohodek j-te družine v k-tem stratumu.\\
%Povprečni dohodek družine se sedaj izraža kot:
%\begin{equation*}
%\overline{X} = \frac{1}{n} \sum_{k=1}^{\# stratumov} \sum_{j=1}^{n_k} X_{kj}.
%\end{equation*}
%Standarna napaka $se(\overline{X}) = \sqrt{var( \overline{X})}$:
%\begin{equation*}
%var(\overline{X}) = \sum_k w_k^2 var(\overline{X_k}))  = \sum_k w_k^2 \cdot \frac{\hat{\sigma}_k^2}{n_k} \cdot \frac{N_k-n_k}{N_k-1},
%\end{equation*}
%kjer je $w_k = \frac{N_k}{N}$ delež, $\sigma_k^2$ pa populacijska varjanca v k-tem stratumu. Torej:
%\begin{equation*}
%\hat{\sigma}_k^2 = \frac{N_k-1}{N_k(n_k-1)} \sum_{j=1}^{n_k} (X_{kj}-\overline{X_k})^2,
%\end{equation*}
%kjer je $X_k$ povprečje k-tega stratuma.\\
Povprečje in standarno napako računamo po stratumih in jih primerno obtežimo, da dobimo skupno povprečje in standarno napako.\\
Interval zaupanja: $\overline{X} \pm 1,96 \cdot se(\overline{X})$. \\
Vstavimo podatke in dobimo:\\
Ocena povprečja dohodka, če stratificiramo: 41215.59575702422.\\
Standarna napaka, če stratificiramo: 16269.87055983993.\\
Interval zaupanja, če stratificiramo: [9326.649459737953,73104.54205431048].\\
Vidimo, da je standardna napaka v tem primeru veliko večja, zato rečemo, da se ne splača stratificirati.

\subsection*{c)}
Variance znotraj četrti:\\
 Prva: 1207934484.6014097, \\
 Druga: 647823289.4396416, \\
 Tretja: 962434398.3292009, \\
 Četrta: 1035070492.6067415.\\
Variance med četrtmi dobimo tako, da izračunamo povprečje dohodka v vsaki četrti, jo primerno obtežimo in izračunamo varjanco. Torej dobimo:\\
Varianca med četrtmi: 2162215.4374019047.

\section*{2. NALOGA}
\subsubsection*{a)}
Za rešitev te naloge, sem si pomagala s knjigo \textit{John A. Rice: Mathematical Statistic and Data Analysis.}
Poglavje 7.5.2, stran 230, nam pove da je:
\begin{equation*}
var(\overline{X}_s) \approx \sum_{l=1}^L \frac{W_l^2 \sigma_l^2}{n_l}.
\end{equation*}
Uporabimo izrek (Theorem A) iz poglavja 7.5.3 na strani 232. \\
\textbf{Izrek}: Velikosti vzorcev $n_1,...,n_L$, ki minimizirajo $var(\overline{X}_s)$, pri pogoju $n_1+\cdots + n_L = n$, so podane z naslednjo enačbo:

\begin{equation*}
n_l = n\frac{W_l \sigma_l}{\sum_{k=1}^L W_k\sigma_k},
\end{equation*}
kjer je l = 1,...,L.\\

Od tod direktno sledi, da so naše minimalne vrednosti:
\begin{equation*}
n_i = n \frac{W_i \sigma_i}{\sum_{l=1}^K W_l \sigma_l}
\end{equation*}


\subsubsection*{b)}
Ta del naloge je podoben a) primeru, le da je naš začetni pogoj drugačen. Zato sledimo dokazu zgornjega izreka, le da spremenimo pogoj. Dokaz je na strani 233, poglavje 7.5.3.\\
Uporabimo tako imenovani vezani ekstrem ali Lagrangev multiplikator.
\begin{equation*}
L(n_1,...,n_K, \lambda) = \sum_{k=1}^K \frac{W_k^2\sigma_k^2}{n_k} + \lambda \left(  C_0 + \sum_{k=1}^K n_k C_k - C\right)
\end{equation*}
Iščemo ekstreme, zato parcialno odvajamo L.
\begin{equation*}
\frac{\partial L}{\partial n_k} = -\frac{W_k\sigma^2_k}{n_k^2} + \lambda C_k,
\end{equation*}
za $k = 1,2,...,K$.\\
Vse parcialne odvode enačimo z 0 in dobimo:
\begin{equation*}
n_k = \frac{W_k\sigma_k}{\sqrt{\lambda C_k}}
\end{equation*}
Vemo: $n = n_1+\cdots + n_K$. Torej:
\begin{equation*}
n = \sum_{k=1}^K n_k = \sum_{k=1}^K \frac{W_k\sigma_k}{\sqrt{\lambda C_k}} = \frac{1}{\sqrt{\lambda}}\sum_{k=1}^K \frac{W_k\sigma_k}{\sqrt{C_k}}
\end{equation*}
Sledi:
\begin{equation*}
\sqrt{\lambda }=   \frac{1}{n} \sum_{k=1}^K \frac{W_k\sigma_k}{\sqrt{C_k}} 
\end{equation*}
Končno sledi:
\begin{equation*}
n_k = \frac{nW_k\sigma_k}{\sqrt{C_k}\sum_{i=1}^K \frac{W_i\sigma_i}{\sqrt{C_i}}}
\end{equation*}

\subsubsection*{c)}
V tem primeru želimo minimizirati stroške. Torej bomo zapisali vezani ekstrem za stroške raziskave, pri pogoju $var(\overline{X}_s) = \sum_{k=1}^K \frac{W_k^2\sigma_k^2}{n_k}$. Torej:
\begin{equation*}
L(n_1,...,n_K,\lambda) = C_0 + \sum_{k=1}^KC_kn_k + \lambda \left(  \sum_{k=1}^K \frac{W_k^2\sigma_k^2}{n_k} - var(\overline{X}_s) \right)
\end{equation*}
Postopamo kot v prejšnjih točkah:
\begin{equation*}
\frac{\partial L}{\partial n_i} = C_i - \lambda \frac{W_i^2\sigma_i^2}{n_i^2} = 0,
\end{equation*}
za $i = 1,2,...,K$.\\
Računamo:
\begin{equation*}
n_i = \sqrt{\frac{\lambda}{C_i}} W_i \sigma_i.
\end{equation*}

\begin{equation*}
n = \sum_{i = 1}^K n_i = \sum_{i=1}^K \sqrt{\frac{\lambda}{C_i}} W_i \sigma_i = \sqrt{\lambda}\sum_{i=1}^K \sqrt{\frac{1}{C_i}} W_i \sigma_i
\end{equation*}

\begin{equation*}
\sqrt{\lambda}  =\frac{n}{\sum_{i=1}^K \sqrt{\frac{1}{C_i}} W_i \sigma_i}
\end{equation*}
Torej:
\begin{equation*}
n_i = \frac{n}{\sqrt{C_i}\sum_{j=1}^K \sqrt{\frac{1}{C_j}} W_j \sigma_j} W_i \sigma_i.
\end{equation*}



\section*{3.NALOGA}
\subsection*{a)}
\begin{equation*}
X\sim f(x,\alpha) =\begin{cases}
 \frac{\Gamma(3\alpha)}{\Gamma(\alpha)\Gamma(2\alpha)} x^{\alpha-1}(1-x)^{2\alpha-1}, & 0<x<1.\\
0, & sicer.
\end{cases}
\end{equation*}
Vemo: 
\begin{equation*}
E(X) = \frac{1}{3}, var(X) = \frac{2}{9(3\alpha+1)}.
\end{equation*}
Ker je:
\begin{equation*}
var(X)=E(X^2)-E(X)^2,
\end{equation*}
sledi: 
\begin{equation*}
E(X^2) = var(X) + E(X)^2 = \frac{2}{9(3\alpha+1)} + \frac{1}{9} = 
\frac{\alpha +1}{3(3\alpha+1)}
\end{equation*}
$E(X^2)$ je drugi moment slučajne spremenljivke X, torej:
\begin{equation*}
E(X^2) = \frac{1}{n} \sum_{i=1}^n X_i^2.
\end{equation*}
Zgornji enačbi izenačimo in poračunamo $\alpha$:
\begin{equation*}
 \frac{1}{n} \sum_{i=1}^n X_i^2 = \frac{\alpha +1}{3(3\alpha+1)}
\end{equation*}
\begin{equation*}
\frac{9\alpha+3}{n} \sum_{i=1}^n X_i^2 = \alpha +1
\end{equation*}
\begin{equation*}
\frac{9\alpha}{n} \sum_{i=1}^n X_i^2  - \alpha = 1 - \frac{3}{n}\sum_{i=1}^n X_i^2
\end{equation*}
\begin{equation*}
\alpha = \frac{1 - \frac{3}{n}\sum_{i=1}^n X_i^2}{\frac{9}{n} \sum_{i=1}^n X_i^2  - 1}
\end{equation*}

\subsection*{b)}
\begin{equation*}
 f_X(x,\alpha) =\begin{cases}
 \frac{\Gamma(3\alpha)}{\Gamma(\alpha)\Gamma(2\alpha)} x^{\alpha-1}(1-x)^{2\alpha-1}, & 0<x<1.\\
0, & sicer.
\end{cases}
\end{equation*}

\begin{equation*}
L_1(\alpha|x) =  \frac{\Gamma(3\alpha)}{\Gamma(\alpha)\Gamma(2\alpha)} x^{\alpha-1}(1-x)^{2\alpha-1}
\end{equation*}

\begin{equation*}
\begin{split}
L(\alpha|x_1,...,x_n) =  L_1(\alpha|x_1) \cdot ... \cdot L_1(\alpha|x_n) =             \\ 
 \left( \frac{\Gamma(3\alpha)}{\Gamma(\alpha)\Gamma(2\alpha)} \right)^n x_1^{\alpha-1} \cdot ... \cdot x_n^{\alpha-1} (1-x_1)^{2\alpha-1} \cdot ... \cdot (1-x_n)^{2\alpha-1}
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
l(\alpha|x_1,...,x_n) = ln( L(\alpha|x_1,...,x_n)) = 
l_1(\alpha|x_1) + ... + l_1(\alpha|x_n)
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
l_1(\alpha|x) = ln(L_1(\alpha|x)) =\\ ln(\Gamma(3\alpha)) - ln(\Gamma(\alpha)) - ln(\Gamma(2\alpha)) + (\alpha-1)ln(x) + (2\alpha-1)ln(1-x)
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
l(\alpha|x) =\\ \sum_{i=1}^n\left( ln(\Gamma(3\alpha)) - ln(\Gamma(\alpha)) - ln(\Gamma(2\alpha)) + (\alpha-1)ln(x_i) + (2\alpha-1)ln(1-x_i) \right)
\end{split}
\end{equation*}

\begin{equation*}
\frac{\partial l}{\partial \alpha} = \sum_{i = 1}^n \frac{1}{\Gamma(3\alpha)} \Gamma^{'}(3\alpha) 3 -
\frac{1}{\Gamma(\alpha)} \Gamma^{'}(\alpha) - \frac{1}{\Gamma(2\alpha)} \Gamma^{'}(2\alpha) 2 +
ln(x_i) + 2ln(1-x_i)
\end{equation*}

\begin{equation*}
\begin{split}
\sum_{i = 1}^n \frac{1}{\Gamma(3\alpha)} \Gamma^{'}(3\alpha) 3 -
\frac{1}{\Gamma(\alpha)} \Gamma^{'}(\alpha) - \frac{1}{\Gamma(2\alpha)} \Gamma^{'}(2\alpha) 2 =
ln(\frac{1}{x_i(1-x_i)})
\end{split}
\end{equation*}
Cenilka obstaja, ko ima zgornja enačba rešitev.\\

\subsection*{c)}
$var(\hat{\alpha}) = \frac{1}{nI_1(\hat{\alpha})}$.
\begin{equation*}
I_1(\hat{\alpha}) = -E\left[  \frac{\partial^2l_1(\alpha|x)}{\partial \alpha^2}   \right]
\end{equation*}

\begin{equation*}
\begin{split}
\frac{\partial^2l_1(\alpha|x)}{\partial \alpha^2} =\\  3\frac{\Gamma^{''}(3\alpha)\Gamma(3\alpha) - \Gamma^{'}(3\alpha)^2}{\Gamma(3\alpha)^2} -
\frac{\Gamma^{''}(\alpha)\Gamma(\alpha) - \Gamma^{'}(\alpha)^2}{\Gamma(\alpha)^2} -
2\frac{\Gamma^{''}(2\alpha)\Gamma(2\alpha) - \Gamma^{'}(2\alpha)^2}{\Gamma(2\alpha)^2}
\end{split}
\end{equation*}

\begin{equation*}
var(\hat{\alpha}) =\frac{1}{n} \frac{1}{ 3\frac{\Gamma^{''}(3\alpha)\Gamma(3\alpha) - \Gamma^{'}(3\alpha)^2}{\Gamma(3\alpha)^2} -
\frac{\Gamma^{''}(\alpha)\Gamma(\alpha) - \Gamma^{'}(\alpha)^2}{\Gamma(\alpha)^2} -
2\frac{\Gamma^{''}(2\alpha)\Gamma(2\alpha) - \Gamma^{'}(2\alpha)^2}{\Gamma(2\alpha)^2}}
\end{equation*}


\section*{4.NALOGA}
Za reševanje te naloge, sem si pomagala s knjigo \textit{John A. Rice: Mathematical Statistic and Data Analysis.}. Iz poglavja 9.5, na strani 341, izvemo, da če imamo model, kjer so verjetnosti vektorsko porazdeljene in imamo opis za hipotezo $H_0$, kjer je $p = p(\theta)$, kjer je $\theta$ neznam, je števec za verjetnostno funkcijo enak:
\begin{equation*}
\max_{p\in \omega_0} \left(  \frac{n!}{x_1!...x_m!} \right) p_1(\theta)^{x_1} \cdots p_m(\theta)^{x_m},
\end{equation*}
kjer so $x_i$ opazovane vrednosti v $m$ celicah.
Maksimum bo dosežen pri:
\begin{equation*}
\hat{p_i} =  \frac{x_i}{n}.
\end{equation*}
Razmerje verjetij je tako enako:
\begin{equation*}
\Lambda= \frac{\frac{n!}{x_1!...x_m!}  p_1(\hat{\theta})^{x_1} \cdots p_m(\hat{\theta})^{x_m}}{\frac{n!}{x_1!...x_m!}  \hat{p_1}^{x_1} \cdots \hat{p_m}^{x_m}}.
\end{equation*}

Poglejmo sedaj naš primer. Podatke imamo podane v razpredelnici za $m=0,1,...,12.$, $n = \text{\# podatkov} = 6115.$ Oprazovane vrednosti $x_1,...,n_{12}$ prebermo iz drugega stolpca v tabeli. Računamo po postopku opisanem zgoraj:
\begin{equation*}
\max \left( \frac{6115!}{7! 45! 181! \cdots 24!3!}  \right) p_1^7 p_2^{45}\cdots p_{12}^3
\end{equation*}
Maksimum je torej dosežen pri:
\begin{equation*}
\hat{p}_1 = \frac{7}{6115}, \hat{p}_2 = \frac{45}{6115}, \cdots, \hat{p}_{12} = \frac{3}{6115}, 
\end{equation*}
Poračunati moramo še imenovalec ulomka, da bomo dobili razmerje verjetij. Testirati želimo hipotezo, da je število moških potomcov, ki se rodijo v družini z 12 otroci, porazdeljeno binomsko $Bin(12,p)$. Torej je verjetnostna funkcija L enaka:
\begin{equation*}
L = \left( \frac{6115!}{7! 45! 181! \cdots 24!3!}  \right) \left( (1-p)^{12} \right)^7 \left( 2p(1-p)^{11}\right)^{45} \cdots \left( p^{12}\right)^3.
\end{equation*}
Izraz logaritmiramo in odvajamo po p, izračunamo maksimum.
\begin{equation*}
\frac{\partial l}{\partial p} = 0.
\end{equation*}
Po krajšem izračunu dobimo:
\begin{equation*}
\frac{35280}{p} = \frac{38100}{1-p}
\end{equation*}
Tako dobimo:
\begin{equation*}
\hat{p} = \frac{35280}{73380} = 0.4807849550286182.
\end{equation*}
Torej po Wilkinsonovem izreku zapišemo:
\begin{equation*}
\lambda = 2ln(\Lambda) = 2 l(\hat{p}_0,...,\hat{p}_{12}) - 2l(\hat{p})
\end{equation*}
Vstavimo zgoraj poračunane vrednosti:
\begin{equation*}
2\left( 7ln(\hat{p}_1) + 45ln(\hat{p}_2)\cdots 3ln(\hat{p}_{12}) \right)- 2 \left( 7*12ln(1-\hat{p}) + 45(ln(12\hat{p})+11ln(1-\hat{p})) + \cdots + 3*12ln(\hat{p})\right)
\end{equation*}
Ko vstavimo podatke dobimo:
\begin{equation*}
\lambda = 97.0065.
\end{equation*}
Wilksov izrek nam pove, da bomo hipotezo zavrnili, če je $\lambda > \chi^2_{1-\alpha}(dim\Omega - dim\Omega_0)$.
Dimenziji prostorov sta očitni: $dim\Omega - dim\Omega_0 = 12-1 = 11$. Testitamo za $\alpha = 0.01 $ in $\alpha = 0.05$. Iz tabele preberemo vrednosti in dobimo:
\begin{equation*}
\chi^2_{0.99}(11) = 19.68, \chi^2_{0.95}(11) = 24.72.
\end{equation*}
Vidimo, da sta vrednosti v obeh primerih manjši od $\lambda$, zato hipotezo v obeh primerih zavrnemo.\\
Model bi lahko bil napačen iz več razlogov. Eden bi bil naprimer, da nismo upoštevali vsa rojstva, vendar le preživele otroke. Lahko, da smo zbrali podatke v različnih zgodovinskih obdobjih, katera vplivajo na rojstva otrok (vojna in podobno).

\section*{5.NALOGA}
\subsubsection*{Ocen po metodi največjega verjetja}
Da bomo vedeli kako so porazdeljeni $Y_i$-ji, poračunamo njihovo pričakovano vrednost in varjanco.
\begin{equation*}
E(Y_i) = E(\beta_0+\beta_1x_i+\epsilon_i) = \beta_0+\beta_1x_1
\end{equation*}

\begin{equation*}
var(Y_i) = \sigma^2
\end{equation*}

Torej so $Y_i$ porazdeljeni $N( \beta_0+\beta_1x_1, \sigma^2)$.
Dobimo:
\begin{equation*}
L_1 = \frac{1}{\sqrt{2\pi \sigma^2}}e^{-{\frac{(y_i - \beta_o-\beta_1x_i)^2}{2\sigma^2}}}
\end{equation*}
Verjetnostna funkcija je sestavljena iz produktov porazdelitvenih funkcij. Torej dobimo:
\begin{equation*}
log(L) = -nlog(\sigma) - \frac{n}{2}log(2\pi)-\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\beta_o-\beta_1x_i)^2
\end{equation*}.
Najprej poračunamo za prvi parameter $\beta_0$. Enačbo odvajamo po $\beta_0$ in enačimo z 0.

\begin{equation*}
\begin{split}
\frac{\partial}{\partial\beta_0} log(L) = \frac{\partial}{\partial\beta_0} (  -nlog(\sigma) - \frac{n}{2}log(2\pi)-\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\beta_o-\beta_1x_i)^2)=\\
= \frac{1}{\sigma^2}\sum_{i=1}^n(y_i-\beta_o-\beta_1x_i)
\end{split}
\end{equation*}
Dobimo:
\begin{equation*}
\sum_{i=1}^n(y_i-\beta_o-\beta_1x_i) = 0
\end{equation*}
Sledi:
\begin{equation*}
\sum_{i=1}^n y_i - n\beta_0 -\beta_1\sum_{i=1}^nx_i = 0
\end{equation*}
Torej je končni rezultat enak:
\begin{equation*}
\beta_o = \frac{1}{n}\left( \sum_{i=1}^n y_i - \beta_1\sum_{i=1}^nx_i \right).
\end{equation*}

Postopek ponovimo za drugi parameter $\beta_1$:
\begin{equation*}
\begin{split}
\frac{\partial}{\partial\beta_1} log(L) = \frac{\partial}{\partial\beta_1} (  -nlog(\sigma) - \frac{n}{2}log(2\pi)-\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i-\beta_o-\beta_1x_i)^2)=\\
= \frac{1}{\sigma^2}\sum_{i=1}^n x_i(y_i-\beta_0-\beta_1x_i)
\end{split}
\end{equation*}

Dobimo:
\begin{equation*}
\sum_{i=1}^nx_i(y_i-\beta_o-\beta_1x_i) = 0.
\end{equation*}

Sledi:
\begin{equation*}
\sum_{i=1}^nx_iy_i-\beta_o\sum_{i=1}^nx_i-\beta_1\sum_{i=1}^nx_i^2 = 0.
\end{equation*}

Vstavimo vrednost za $\beta_0$:
\begin{equation*}
\sum_{i=1}^nx_iy_i-\frac{1}{n}\left( \sum_{i=1}^n y_i - \beta_1\sum_{i=1}^nx_i \right)\sum_{i=1}^nx_i-\beta_1\sum_{i=1}^nx_i^2 = 0.
\end{equation*}
\begin{equation*}
\sum_{i=1}^nx_iy_i- \frac{1}{n}\sum_{i=1}^n y_i \sum_{i=1}^n x_i- \beta_1\left(-\frac{1}{n}\sum_{i=1}^nx_i\sum_{i=1}^nx_i+\sum_{i=1}^nx_i^2\right) = 0.
\end{equation*}


Torej je končni rezultat enak:
\begin{equation*}
\beta_1 = \frac{\sum_{i=1}^nx_iy_i- \frac{1}{n}\sum_{i=1}^n y_i \sum_{i=1}^n x_i}{-\frac{1}{n}\sum_{i=1}^nx_i\sum_{i=1}^nx_i+\sum_{i=1}^nx_i^2}
\end{equation*}
Oziroma:
\begin{equation*}
\beta_1 = \frac{n\sum_{i=1}^nx_iy_i- \sum_{i=1}^n y_i \sum_{i=1}^n x_i}{n\sum_{i=1}^nx_i^2-\sum_{i=1}^nx_i\sum_{i=1}^nx_i}
\end{equation*}

Vstavimo še za $\beta_0$:
\begin{equation*}
\beta_o = \frac{1}{n} \left(\sum_{i=1}^n y_i - \frac{n\sum_{i=1}^nx_iy_i- \sum_{i=1}^n y_i \sum_{i=1}^n x_i}{n\sum_{i=1}^nx_i^2-\sum_{i=1}^nx_i\sum_{i=1}^nx_i}\sum_{i=1}^nx_i \right).
\end{equation*}
\begin{equation*}
\beta_o = \frac{1}{n} \left(\sum_{i=1}^n y_i - \frac{n\sum_{i=1}^nx_iy_i \sum_{i=1}^nx_i - \sum_{i=1}^n y_i \sum_{i=1}^n x_i\sum_{i=1}^nx_i }{n\sum_{i=1}^nx_i^2-\sum_{i=1}^nx_i\sum_{i=1}^nx_i}\right).
\end{equation*}
\begin{equation*}
\beta_o =  \frac{1}{n}\frac{n\sum_{i=1}^n y_i \sum_{i=1}^n x_i^2 - \sum_{i=1}^ny_i\sum_{i=1}^nx_i\sum_{i=1}^nx_i-n\sum_{i=1}^nx_iy_i \sum_{i=1}^nx_i + \sum_{i=1}^n y_i \sum_{i=1}^n x_i\sum_{i=1}^nx_i }{n\sum_{i=1}^nx_i^2-\sum_{i=1}^nx_i\sum_{i=1}^nx_i}.
\end{equation*}
Končni rezultat:
\begin{equation*}
\beta_o =  \frac{\sum_{i=1}^n y_i \sum_{i=1}^n x_i^2 -\sum_{i=1}^nx_iy_i \sum_{i=1}^nx_i}{n\sum_{i=1}^nx_i^2-\sum_{i=1}^nx_i\sum_{i=1}^nx_i}.
\end{equation*}


\subsubsection*{Ocena po metodi najmanjših kvadratov}
Ker so šumi $\epsilon_i$ neodvisni za vsak $i = 1,2,...,n$ in $\epsilon_i \sim N(0,\sigma^2)$ lahko uporabimo izrek Gauss-Markova za ocene parametrov $\beta_0$ in $\beta_1$.
Imamo:
\begin{equation*}
\begin{bmatrix}
y_1\\
\vdots\\
y_n
\end{bmatrix}
=
\begin{bmatrix}
1 & x_1\\
\vdots &\vdots \\
1 & x_n
\end{bmatrix}
\begin{bmatrix}
\beta_0\\
\beta_1
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_1\\
\vdots\\
\epsilon_n
\end{bmatrix}
\end{equation*}

Po izreku Gauss-Markova vemo da je najboljša cenilka za $\beta = (X^TX)^{-1}X^TY$. Računamo:

\begin{equation*}
\left(
\begin{bmatrix}
1 & \cdots & 1\\
x_1 & \cdots & x_n
\end{bmatrix}
\begin{bmatrix}
1 & x_1\\
\vdots &\vdots \\
1 & x_n
\end{bmatrix}
\right)^{-1}
\begin{bmatrix}
1 & \cdots & 1\\
x_1 & \cdots & x_n
\end{bmatrix}
\begin{bmatrix}
y_1\\
\vdots\\
y_n
\end{bmatrix}
=
\end{equation*}

\begin{equation*}
=
\left(
\begin{bmatrix}
n & \sum_{i=1}^nx_i\\
\sum_{i=1}^nx_i & \sum_{i=1}^nx_i^2
\end{bmatrix}
\right)^{-1}
\begin{bmatrix}
\sum_{i=1}^ny_i\\
\sum_{i=1}^nx_iy_i
\end{bmatrix}
=
\end{equation*}

\begin{equation*}
=
\frac{1}{n\sum_{i=1}^nx_i^2-\sum_{i=1}^nx_i\sum_{i=1}^nx_i}
\begin{bmatrix}
\sum_{i=1}^nx_i^2 & -\sum_{i=1}^nx_i\\
-\sum_{i=1}^nx_i & n
\end{bmatrix}
\begin{bmatrix}
\sum_{i=1}^ny_i\\
\sum_{i=1}^nx_iy_i
\end{bmatrix}
=
\end{equation*}

\begin{equation*}
=
\frac{1}{n\sum_{i=1}^nx_i^2-\sum_{i=1}^nx_i\sum_{i=1}^nx_i}
\begin{bmatrix}
\sum_{i=1}^nx_i^2\sum_{i=1}^ny_i - \sum_{i=1}^nx_i\sum_{i=1}^nx_iy_i\\
-\sum_{i=1}^nx_i\sum_{i=1}^ny_i+n\sum_{i=1}^nx_iy_i
\end{bmatrix}
\end{equation*}

Zgornjo vrstico primerjamo z $\beta_0$, ki smo jo dobili po metodi največjega verjetja, spodnjo pa z $\beta_1$. Vidimo, da sta enaki.







































\end{document}